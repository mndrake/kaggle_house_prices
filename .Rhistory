library(readr)
library(cliffnotes)
train <- read_csv('input/train.csv')
cliffnotes(train)
lm(SalePrice ~ LotArea, data = train)
model <- lm(SalePrice ~ LotArea, data = train)
plot(model)
plot(model)
model <- lm(log(SalePrice) ~ LotArea, data = train)
plot(model)
plot(model)
model <- glm(SalePrice ~ ., data = train)
train <- read.csv('input/train.csv')
train$Id <- NULL
cliffnotes(train)
train$target <- log(train$SalePrice)
train$SalePrice <- NULL
cliffnotes(train)
model <- glm(SalePrice ~ ., data = train)
model <- glm(target ~ ., data = train)
str(train)
colname <- colnames(train)[[1]]
train[colname]
class(train[colname])
class(train[[colname])
class(train[[colname]])
for (colname in colnames(train)) {
if is.numeric(train[[colname]]) {
cat(paste(colname, 'numeric'))
} else {
cat(paste(colname, 'other'))
}
}
for (colname in colnames(train)) {
if is.numeric(train[[colname]]) {
cat(paste(colname, 'numeric'))
} else {
cat(paste(colname, 'other'))
}
}
for (colname in colnames(train)) {
#   if is.numeric(train[[colname]]) {
#     cat(paste(colname, 'numeric'))
#   } else {
#     cat(paste(colname, 'other'))
#   }
}
for (colname in colnames(train)) {
print(colname)
#   if is.numeric(train[[colname]]) {
#     cat(paste(colname, 'numeric'))
#   } else {
#     cat(paste(colname, 'other'))
#   }
}
for (colname in colnames(train)) {
if is.numeric(train[[colname]]) {
#     cat(paste(colname, 'numeric'))
} # else {
#     cat(paste(colname, 'other'))
#   }
}
for (colname in colnames(train)) {
if (is.numeric(train[[colname]])) {
#     cat(paste(colname, 'numeric'))
} # else {
#     cat(paste(colname, 'other'))
#   }
}
for (colname in colnames(train)) {
if (is.numeric(train[[colname]])) {
cat(paste(colname, 'numeric'))
} # else {
#     cat(paste(colname, 'other'))
#   }
}
for (colname in colnames(train)) {
if (is.numeric(train[[colname]])) {
cat(paste(colname, 'numeric \n'))
} # else {
#     cat(paste(colname, 'other'))
#   }
}
for (colname in colnames(train)) {
if (is.numeric(train[[colname]])) {
cat(paste0(colname, ': numeric \n'))
} # else {
#     cat(paste(colname, 'other'))
#   }
}
for (colname in colnames(train)) {
if (is.numeric(train[[colname]])) {
cat(paste0(colname, ': numeric \n'))
}  else {
cat(paste0(colname, ': other \n'))
}
}
train['GarageYrBlt']
mean(train['GarageYrBlt'], na.rm = TRUE)
mean(train['GarageYrBlt'], na.rm = TRUE)
mean(train['GarageYrBlt'])
mean(train[['GarageYrBlt']])
mean(train[['GarageYrBlt']], na.rm = TRUE)
train['GarageYrBlt']
train['GarageYrBlt']
train[,'GarageYrBlt'] == NA
train[['GarageYrBlt']] == NA
train[,colname][[train[[colname]] == NA]
train[,colname][train[[colname]] == NA]
mean(train[[colname]], na.rm = TRUE)
train[,colname][train[[colname]] == NA]
train[,colname][train[[colname]] == NA]
train[,colname][train[[colname]] == NA] <- mean(train[[colname]], na.rm = TRUE)
for (colname in colnames(train)) {
if (is.numeric(train[[colname]])) {
train[,colname][train[[colname]] == NA] <- mean(train[[colname]], na.rm = TRUE)
}
}
model <- glm(target ~ ., data = train)
train
is.na(train)
train[is.na(train)]
train[is.na(train)] <- 'NA'
model <- glm(target ~ ., data = train)
str(train)
train <- read.csv('input/train.csv')
str(train)
train$Id <- NULL
train$target <- log(train$SalePrice)
train$SalePrice <- NULL
str(train)
cliffnotes(train)
for (colname in colnames(train)) {
if (is.numeric(train[[colname]])) {
train[,colname][train[[colname]] == NA] <- mean(train[[colname]], na.rm = TRUE)
}
}
str(train)
model <- glm(target ~ ., data = train)
str(train)
for (colname in colnames(train)) {
if (is.numeric(train[[colname]])) {
colname
}
}
for (colname in colnames(train)) {
if (is.numeric(train[[colname]])) {
print(colname)
}
}
for (colname in colnames(train)) {
if (is.numeric(train[[colname]])) {
cat(paste0(colname, ' + '))
}
}
model <- glm(target ~ MSSubClass + LotFrontage + LotArea + OverallQual + OverallCond +
YearBuilt + YearRemodAdd + MasVnrArea + BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF +
TotalBsmtSF + X1stFlrSF + X2ndFlrSF + LowQualFinSF + GrLivArea + BsmtFullBath +
BsmtHalfBath + FullBath + HalfBath + BedroomAbvGr + KitchenAbvGr + TotRmsAbvGrd +
Fireplaces + GarageYrBlt + GarageCars + GarageArea + WoodDeckSF + OpenPorchSF +
EnclosedPorch + X3SsnPorch + ScreenPorch + PoolArea + MiscVal + MoSold + YrSold,
data = train)
model
plot(model)
model$coefficients
model$effects
summary(model)
plot(model)
train[1299,]
library(h2o)
train <- h2o.importFile('input/train.csv')
library(h2o)
h2o.init()
train <- h2o.importFile('input/train.csv')
train
train$target <- log(train$SalePrice)
X <- setdiff(colnames(train), 'Id','SalePrice')
y <- 'target'
X <- setdiff(colnames(train), c('Id','SalePrice'))
y <- 'target'
model <- h2o.deeplearning(X, y, training_frame = train, model_id = 'dl_01', overwrite_with_best_model = TRUE,
use_all_factor_levels = TRUE, standardize = TRUE, activation = "RectifierWithDropout",
hidden = c(200,200), epochs = 10, variable_importances = TRUE, fold_assignment = "AUTO",
train_samples_per_iteration = -2, adaptive_rate = TRUE, input_dropout_ratio = 0.1,
l1 = 0.001, l2 = 0, loss = "Huber", distribution = "AUTO", huber_alpha = 0.9,
score_interval = 5, score_training_samples = 10000, score_duty_cycle = 0.1,
stopping_rounds = 5, stopping_metric = "MSE", stopping_tolerance = 0.001,
categorical_encoding = "AUTO", target_ratio_comm_to_comp = 0.05, seed=1234,
rho = 0.99, epsilon = 1e-8, nesterov_accelerated_gradient = TRUE, max_w2 = "Infinity",
initial_weight_distribution = "UniformAdaptive", nfolds = 10)
model <- h2o.deeplearning(X, y, training_frame = train, model_id = 'dl_01',
overwrite_with_best_model = TRUE,
use_all_factor_levels = TRUE,
standardize = TRUE, activation = "RectifierWithDropout",
hidden = c(200,200), epochs = 10, variable_importances = TRUE, fold_assignment = "AUTO",
train_samples_per_iteration = -2, adaptive_rate = TRUE, input_dropout_ratio = 0.1,
l1 = 0.001, l2 = 0, loss = "Huber", distribution = "AUTO", huber_alpha = 0.9,
score_interval = 5, score_training_samples = 10000, score_duty_cycle = 0.1,
stopping_rounds = 5, stopping_metric = "MSE", stopping_tolerance = 0.001,
categorical_encoding = "AUTO", target_ratio_comm_to_comp = 0.05, seed=1234,
rho = 0.99, epsilon = 1e-8, nesterov_accelerated_gradient = TRUE,
initial_weight_distribution = "UniformAdaptive", nfolds = 10)
model
h2o.rmse(model)
h2o.rmse(model, xval = T)
h2o.rmse(model, xval = TRUE)
model <- h2o.deeplearning(X, y, training_frame = train, model_id = 'dl_01',
overwrite_with_best_model = TRUE,
use_all_factor_levels = TRUE,
standardize = TRUE, activation = "RectifierWithDropout",
hidden = c(200,200), epochs = 10, variable_importances = TRUE, fold_assignment = "AUTO",
train_samples_per_iteration = -2, adaptive_rate = TRUE, input_dropout_ratio = 0.1,
l1 = 0.0001, l2 = 0, loss = "Huber", distribution = "AUTO", huber_alpha = 0.9,
score_interval = 5, score_training_samples = 10000, score_duty_cycle = 0.1,
stopping_rounds = 5, stopping_metric = "MSE", stopping_tolerance = 0.001,
categorical_encoding = "AUTO", target_ratio_comm_to_comp = 0.05, seed=1234,
rho = 0.99, epsilon = 1e-8, nesterov_accelerated_gradient = TRUE,
initial_weight_distribution = "UniformAdaptive", nfolds = 10,
regression_stop = 0.000001, diagnostics = TRUE, fast_mode = TRUE,
force_load_balance = TRUE, single_node_mode = FALSE, shuffle_training_data = FALSE,
missing_values_handling = "MeanImputation", quiet_mode = FALSE,
sparse = FALSE, col_major = FALSE, average_activation = FALSE, sparsity_beta = 0,
max_categorical_features = 2147483647, reproducible = FALSE, export_weights_and_biases = FALSE)
h2o.rmse(model, xval = TRUE) #0.1292479
model <- h2o.deeplearning(X, y, training_frame = train, model_id = 'dl_01',
overwrite_with_best_model = TRUE,
use_all_factor_levels = TRUE,
standardize = TRUE, activation = "RectifierWithDropout",
hidden = c(200,200), epochs = 10, variable_importances = TRUE, fold_assignment = "AUTO",
train_samples_per_iteration = -2, adaptive_rate = TRUE, input_dropout_ratio = 0.1,
l1 = 0.0001, l2 = 0, loss = "Huber", distribution = "AUTO", huber_alpha = 0.9,
score_interval = 5, score_training_samples = 10000, score_duty_cycle = 0.1,
stopping_rounds = 5, stopping_metric = "MSE", stopping_tolerance = 0.001,
categorical_encoding = "AUTO", target_ratio_comm_to_comp = 0.05, seed=1234,
rho = 0.99, epsilon = 1e-8, nesterov_accelerated_gradient = TRUE,
initial_weight_distribution = "UniformAdaptive", nfolds = 10,
regression_stop = 0.000001, diagnostics = TRUE, fast_mode = TRUE,
force_load_balance = TRUE, single_node_mode = FALSE, shuffle_training_data = FALSE,
missing_values_handling = "MeanImputation", quiet_mode = FALSE,
sparse = FALSE, col_major = FALSE, sparsity_beta = 0,
max_categorical_features = 2147483647, reproducible = FALSE, export_weights_and_biases = FALSE)
h2o.rmse(model, xval = TRUE) #0.1292479
model <- h2o.deeplearning(X, y, training_frame = train, model_id = 'dl_01',
overwrite_with_best_model = TRUE,
use_all_factor_levels = TRUE,
standardize = TRUE, activation = "RectifierWithDropout",
hidden = c(200,200), epochs = 10, variable_importances = TRUE, fold_assignment = "AUTO",
train_samples_per_iteration = -2, adaptive_rate = TRUE, input_dropout_ratio = 0.1,
l1 = 0.01, l2 = 0, loss = "Huber", distribution = "AUTO", huber_alpha = 0.9,
score_interval = 5, score_training_samples = 10000, score_duty_cycle = 0.1,
stopping_rounds = 5, stopping_metric = "MSE", stopping_tolerance = 0.001,
categorical_encoding = "AUTO", target_ratio_comm_to_comp = 0.05, seed=1234,
rho = 0.99, epsilon = 1e-8, nesterov_accelerated_gradient = TRUE,
initial_weight_distribution = "UniformAdaptive", nfolds = 10,
regression_stop = 0.000001, diagnostics = TRUE, fast_mode = TRUE,
force_load_balance = TRUE, single_node_mode = FALSE, shuffle_training_data = FALSE,
missing_values_handling = "MeanImputation", quiet_mode = FALSE,
sparse = FALSE, col_major = FALSE, sparsity_beta = 0,
max_categorical_features = 2147483647, reproducible = FALSE, export_weights_and_biases = FALSE)
h2o.rmse(model, xval = TRUE) #0.1292479
model <- h2o.deeplearning(X, y, training_frame = train, model_id = 'dl_01',
overwrite_with_best_model = TRUE,
use_all_factor_levels = TRUE,
standardize = TRUE, activation = "RectifierWithDropout",
hidden = c(200,200), epochs = 10, variable_importances = TRUE, fold_assignment = "AUTO",
train_samples_per_iteration = -2, adaptive_rate = TRUE, input_dropout_ratio = 0.1,
l1 = 0.001, l2 = 0, loss = "Huber", distribution = "AUTO", huber_alpha = 0.9,
score_interval = 5, score_training_samples = 10000, score_duty_cycle = 0.1,
stopping_rounds = 5, stopping_metric = "MSE", stopping_tolerance = 0.001,
categorical_encoding = "AUTO", target_ratio_comm_to_comp = 0.05, seed=1234,
rho = 0.99, epsilon = 1e-8, nesterov_accelerated_gradient = TRUE,
initial_weight_distribution = "UniformAdaptive", nfolds = 10,
regression_stop = 0.000001, diagnostics = TRUE, fast_mode = TRUE,
force_load_balance = TRUE, single_node_mode = FALSE, shuffle_training_data = FALSE,
missing_values_handling = "MeanImputation", quiet_mode = FALSE,
sparse = FALSE, col_major = FALSE, sparsity_beta = 0,
max_categorical_features = 2147483647, reproducible = FALSE, export_weights_and_biases = FALSE)
h2o.rmse(model, xval = TRUE) #0.1292479
model <- h2o.deeplearning(X, y, training_frame = train, model_id = 'dl_01',
overwrite_with_best_model = TRUE,
use_all_factor_levels = TRUE,
standardize = TRUE, activation = "RectifierWithDropout",
hidden = c(200,200), epochs = 10, variable_importances = TRUE, fold_assignment = "AUTO",
train_samples_per_iteration = -2, adaptive_rate = TRUE, input_dropout_ratio = 0.1,
l1 = 0.0001, l2 = 0, loss = "Huber", distribution = "AUTO", huber_alpha = 0.9,
score_interval = 5, score_training_samples = 10000, score_duty_cycle = 0.1,
stopping_rounds = 5, stopping_metric = "MSE", stopping_tolerance = 0.001,
categorical_encoding = "AUTO", target_ratio_comm_to_comp = 0.05, seed=1234,
rho = 0.99, epsilon = 1e-8, nesterov_accelerated_gradient = TRUE,
initial_weight_distribution = "UniformAdaptive", nfolds = 10,
regression_stop = 0.000001, diagnostics = TRUE, fast_mode = TRUE,
force_load_balance = TRUE, single_node_mode = FALSE, shuffle_training_data = FALSE,
missing_values_handling = "MeanImputation", quiet_mode = FALSE,
sparse = FALSE, col_major = FALSE, average_activation = FALSE, sparsity_beta = 0,
max_categorical_features = 2147483647, reproducible = FALSE, export_weights_and_biases = FALSE)
model <- h2o.deeplearning(X, y, training_frame = train, model_id = 'dl_01',
overwrite_with_best_model = TRUE,
use_all_factor_levels = TRUE,
standardize = TRUE, activation = "RectifierWithDropout",
hidden = c(200,200), epochs = 10, variable_importances = TRUE, fold_assignment = "AUTO",
train_samples_per_iteration = -2, adaptive_rate = TRUE, input_dropout_ratio = 0.1,
l1 = 0.0001, l2 = 0, loss = "Huber", distribution = "AUTO", huber_alpha = 0.9,
score_interval = 5, score_training_samples = 10000, score_duty_cycle = 0.1,
stopping_rounds = 5, stopping_metric = "MSE", stopping_tolerance = 0.001,
categorical_encoding = "AUTO", target_ratio_comm_to_comp = 0.05, seed=1234,
rho = 0.99, epsilon = 1e-8, nesterov_accelerated_gradient = TRUE,
initial_weight_distribution = "UniformAdaptive", nfolds = 10,
regression_stop = 0.000001, diagnostics = TRUE, fast_mode = TRUE,
force_load_balance = TRUE, single_node_mode = FALSE, shuffle_training_data = FALSE,
missing_values_handling = "MeanImputation", quiet_mode = FALSE,
sparse = FALSE, col_major = FALSE, sparsity_beta = 0,
max_categorical_features = 2147483647, reproducible = FALSE, export_weights_and_biases = FALSE)
h2o.rmse(model, xval = TRUE) #0.1292479
library(cliffnotes)
library(readr)
library(cliffnotes)
library(readr)
library(cliffnotes)
library(readr)
train <- read_csv('input/train.csv')
test <- read_csv('input/test.csv')
cliffnotes(train)
library(dplyr)
library(dplyr)
train %>% union_all(test)
combined <- train %>%
union_all(test) %>%
drop(Id, SalePrice)
cliffnotes(train)
cliffnotes(combined)
combined <- train %>%
union_all(test) %>%
drop(Id, SalePrice)
cliffnotes(combined)
combined <- train %>%
union_all(test) %>%
drop(Id, SalePrice)
combined <- train %>%
union_all(test) %>%
drop(c(Id, SalePrice))
combined <- train %>%
union_all(test) %>%
select(-Id, -SalePrice)
cliffnotes(combined)
library(caret)
